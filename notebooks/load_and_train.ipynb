{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import h5py\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variables and model hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X:(24204, 48, 48, 1)\n",
      "Y:(51985, 121)\n"
     ]
    }
   ],
   "source": [
    "# set global variables and hyper-parameters\n",
    "DATA_LOCATION = '../data/'\n",
    "TRAIN_IMAGES_LOCATION = '../data/train_images/'\n",
    "IMAGE_SIZE = 48\n",
    "N_CLASSES = 121\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 15\n",
    "\n",
    "# load train images filenames with class labels\n",
    "filenames = [i for i in os.listdir('../data/train_images') if i.endswith('.jpg')]\n",
    "with open(DATA_LOCATION + 'train_onelabel.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    file_to_class = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "# calculate new class counts, converging towards max (1580)\n",
    "with open(DATA_LOCATION + 'train_onelabel.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    class_counts = {}\n",
    "    for row in reader:\n",
    "        if(row[1] != 'class'):\n",
    "            class_counts[int(row[1])] = class_counts.get(int(row[1]), 0) + 1\n",
    "    max_nr = max(class_counts.values())\n",
    "    for key, value in class_counts.items():\n",
    "        class_counts[key] = int(class_counts[key] + (max_nr - class_counts[key])/6)\n",
    "\n",
    "X = np.empty([len(filenames),IMAGE_SIZE,IMAGE_SIZE,1])\n",
    "Y_tmp = np.empty([len(filenames)])\n",
    "Y = np.empty([sum(class_counts.values()),N_CLASSES])\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padding(i):\n",
    "    \"\"\"\n",
    "    Helper function for getting right padding sizes\n",
    "    input:\n",
    "        - i: positive integer gotten from substracting height and width of an image\n",
    "    output:\n",
    "        - Tuple representing the correct padding\n",
    "    \"\"\"\n",
    "    if i%2 == 0:\n",
    "        return (int(i/2),int(i/2))\n",
    "    else:\n",
    "        return (int(i/2-.5), int(i/2+.5))\n",
    "\n",
    "def pad_image(img):\n",
    "    \"\"\"\n",
    "    Add padding to image to make it square\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "    output:\n",
    "        - padded array of shape (N,N)\n",
    "    \"\"\"\n",
    "    H, W = img.shape\n",
    "    if H == W:\n",
    "        return img\n",
    "    elif H > W:\n",
    "        return np.pad(img, ((0,0), get_padding(H-W)), 'constant')\n",
    "    else:\n",
    "        return np.pad(img, (get_padding(W-H), (0,0)), 'constant')\n",
    "\n",
    "def resize_image(img):\n",
    "    \"\"\"\n",
    "    Resize image to new square shape\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "        - size: final shape of image in pixels (integer)\n",
    "    \"\"\"\n",
    "    return resize(img, (IMAGE_SIZE,IMAGE_SIZE), mode='reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image in filenames:\n",
    "- load file\n",
    "- from [0,255] to [0.0 to 1.0]\n",
    "- square and resize image\n",
    "- either:\n",
    "    - add image once (X & Y)\n",
    "- or:\n",
    "    - rotate [0,90,180,270]\n",
    "    - add 4 images to X\n",
    "    - add 4 labels to Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(filenames)\n",
    "for i in range(len(filenames)):\n",
    "    # read and transform image to usable format\n",
    "    img = mpimg.imread(TRAIN_IMAGES_LOCATION + filenames[i])\n",
    "    img = np.absolute(np.divide(img.astype(float), 255) - 1.0)\n",
    "    img = resize_image(pad_image(img))\n",
    "    # create a grayscale channel \n",
    "    img = img.reshape(IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "    \n",
    "    X[i] = img\n",
    "    Y_tmp[i] = int(file_to_class[filenames[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13c172dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e089e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(16,3))\n",
    "sub1 = plt.subplot(1,4,1)\n",
    "plt.imshow(X[250][:,:,0], cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "Since there is a strong class imbalance (lowest 7, highest 1580), something has to be done to counter this. Oversampling minority classes to be as big as the majority classes is the option used below. <br>\n",
    "(Please do note that the validation split accuracy can not be seen as a surrogate for the test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204, 2304)\n",
      "Shapes:\n",
      "X:(51985, 2304)\n",
      "Y:(51985,)\n",
      "Shapes:\n",
      "X:(51985, 48, 48, 1)\n",
      "Y:(51985, 121)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(total,IMAGE_SIZE*IMAGE_SIZE)\n",
    "print(X.shape)\n",
    "\n",
    "sm = RandomOverSampler(ratio=class_counts)\n",
    "X, Y_tmp = sm.fit_sample(X, Y_tmp)\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y_tmp.shape))\n",
    "\n",
    "X = X.reshape(len(X),IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "for i in range(len(Y_tmp)):\n",
    "    Y[i][int(Y_tmp[i])] = 1.0\n",
    "del(Y_tmp)\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(total,X.shape[0]):\n",
    "    # rotate RandomOverSampler images by one of 0/90/180/270 degrees\n",
    "    X[i] = np.rot90(X[i],(1+(i%4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the lines below appropriately if a validation split is to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "# only uncomment if train/validation variables are used\n",
    "# del(X)\n",
    "# del(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 121)               124025    \n",
      "=================================================================\n",
      "Total params: 12,936,249\n",
      "Trainable params: 12,936,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=X[0].shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16480/51985 [========>.....................] - ETA: 4814s - loss: 15.9298 - acc: 0.0103"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X, \n",
    "    Y,\n",
    "    epochs=N_EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../data/output/models/model10_2x64_2x128_4x256.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=X[0].shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, \n",
    "    Y,\n",
    "    epochs=N_EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../data/output/models/model11_64_32_32_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
