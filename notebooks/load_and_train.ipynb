{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import watershed\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import h5py\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variables and model hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X:(24204, 48, 48, 1)\n",
      "Y:(51985, 121)\n"
     ]
    }
   ],
   "source": [
    "# set global variables and hyper-parameters\n",
    "DATA_LOCATION = '../data/'\n",
    "TRAIN_IMAGES_LOCATION = '../data/train_images/'\n",
    "IMAGE_SIZE = 48\n",
    "N_CLASSES = 121\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 15\n",
    "\n",
    "# load train images filenames with class labels\n",
    "filenames = [i for i in os.listdir('../data/train_images') if i.endswith('.jpg')]\n",
    "with open(DATA_LOCATION + 'train_onelabel.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    file_to_class = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "# calculate new class counts, converging towards max (1580)\n",
    "with open(DATA_LOCATION + 'train_onelabel.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    class_counts = {}\n",
    "    for row in reader:\n",
    "        if(row[1] != 'class'):\n",
    "            class_counts[int(row[1])] = class_counts.get(int(row[1]), 0) + 1\n",
    "    max_nr = max(class_counts.values())\n",
    "    for key, value in class_counts.items():\n",
    "        class_counts[key] = int(class_counts[key] + (max_nr - class_counts[key])/6)\n",
    "\n",
    "X = np.empty([len(filenames),IMAGE_SIZE,IMAGE_SIZE,1])\n",
    "Y_tmp = np.empty([len(filenames)])\n",
    "Y = np.empty([sum(class_counts.values()),N_CLASSES])\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padding(i):\n",
    "    \"\"\"\n",
    "    Helper function for getting right padding sizes\n",
    "    input:\n",
    "        - i: positive integer gotten from substracting height and width of an image\n",
    "    output:\n",
    "        - Tuple representing the correct padding\n",
    "    \"\"\"\n",
    "    if i%2 == 0:\n",
    "        return (int(i/2),int(i/2))\n",
    "    else:\n",
    "        return (int(i/2-.5), int(i/2+.5))\n",
    "\n",
    "def pad_image(img):\n",
    "    \"\"\"\n",
    "    Add padding to image to make it square\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "    output:\n",
    "        - padded array of shape (N,N)\n",
    "    \"\"\"\n",
    "    H, W = img.shape\n",
    "    if H == W:\n",
    "        return img\n",
    "    elif H > W:\n",
    "        return np.pad(img, ((0,0), get_padding(H-W)), 'constant')\n",
    "    else:\n",
    "        return np.pad(img, (get_padding(W-H), (0,0)), 'constant')\n",
    "\n",
    "def resize_image(img):\n",
    "    \"\"\"\n",
    "    Resize image to new square shape\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "        - size: final shape of image in pixels (integer)\n",
    "    \"\"\"\n",
    "    return resize(img, (IMAGE_SIZE,IMAGE_SIZE), mode='reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image in filenames:\n",
    "- load file\n",
    "- from [0,255] to [0.0 to 1.0]\n",
    "- square and resize image\n",
    "- either:\n",
    "    - add image once (X & Y)\n",
    "- or:\n",
    "    - rotate [0,90,180,270]\n",
    "    - add 4 images to X\n",
    "    - add 4 labels to Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X:(24204, 48, 48, 1)\n",
      "Y:(51985, 121)\n"
     ]
    }
   ],
   "source": [
    "total = len(filenames)\n",
    "for i in range(len(filenames)):\n",
    "    # read and transform image to usable format\n",
    "    img = mpimg.imread(TRAIN_IMAGES_LOCATION + filenames[i])\n",
    "    img = np.absolute(np.divide(img.astype(float), 255) - 1.0)\n",
    "    img = resize_image(pad_image(img))\n",
    "    # create a grayscale channel \n",
    "    img = img.reshape(IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "    \n",
    "    X[i] = img\n",
    "    Y_tmp[i] = int(file_to_class[filenames[i]])\n",
    "    \n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x5887d634e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMYAAADHCAYAAABCxyz4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x58a8c47d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(16,3))\n",
    "sub1 = plt.subplot(1,4,1)\n",
    "plt.imshow(X[250][:,:,0], cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "Since there is a strong class imbalance (lowest 7, highest 1580), something has to be done to counter this. Oversampling minority classes to be as big as the majority classes is the option used below. <br>\n",
    "(Please do note that the validation split accuracy can not be seen as a surrogate for the test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204, 2304)\n",
      "Shapes:\n",
      "X:(51985, 2304)\n",
      "Y:(51985,)\n",
      "Shapes:\n",
      "X:(51985, 48, 48, 1)\n",
      "Y:(51985, 121)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(total,IMAGE_SIZE*IMAGE_SIZE)\n",
    "print(X.shape)\n",
    "\n",
    "sm = RandomOverSampler(ratio=class_counts)\n",
    "X, Y_tmp = sm.fit_sample(X, Y_tmp)\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y_tmp.shape))\n",
    "\n",
    "X = X.reshape(len(X),IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "for i in range(len(Y_tmp)):\n",
    "    Y[i][int(Y_tmp[i])] = 1.0\n",
    "del(Y_tmp)\n",
    "print('Shapes:\\nX:{}\\nY:{}'.format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(total,X.shape[0]):\n",
    "    # rotate RandomOverSampler images by one of 0/90/180/270 degrees\n",
    "    X[i] = np.rot90(X[i],(1+(i%4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the lines below appropriately if a validation split is to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "# only uncomment if train/validation variables are used\n",
    "# del(X)\n",
    "# del(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold, dilation & labeling to extract largest non background area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLargestRegion(array):\n",
    "    # Threshold image\n",
    "    imthr = np.where(array < np.mean(array),0.,1.0)\n",
    "\n",
    "    # Dilate image\n",
    "    imdilated = morphology.dilation(imthr, np.ones((2,2)))\n",
    "    \n",
    "    # Label sections\n",
    "    labels = measure.label(imdilated)\n",
    "    labels = imthr*labels\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    # Get largest region\n",
    "    regionmaxprop = None\n",
    "    regions = measure.regionprops(labels)\n",
    "    for regionprop in regions:\n",
    "        # check to see if the region is at least 50% nonzero\n",
    "        if regionmaxprop is None:\n",
    "            regionmaxprop = regionprop\n",
    "        if regionmaxprop.filled_area < regionprop.filled_area:\n",
    "            regionmaxprop = regionprop\n",
    "            \n",
    "    return np.where(labels == regionmaxprop.label,1.0,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transformations to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = getLargestRegion(X[i][:,:,0]).reshape((IMAGE_SIZE,IMAGE_SIZE,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 121)               124025    \n",
      "=================================================================\n",
      "Total params: 12,936,249\n",
      "Trainable params: 12,936,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=X[0].shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16480/51985 [========>.....................] - ETA: 4814s - loss: 15.9298 - acc: 0.0103"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X, \n",
    "    Y,\n",
    "    epochs=N_EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../data/output/models/model10_2x64_2x128_4x256.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 121)               62073     \n",
      "=================================================================\n",
      "Total params: 2,712,889\n",
      "Trainable params: 2,712,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=X[0].shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "51985/51985 [==============================] - 578s 11ms/step - loss: 3.4685 - acc: 0.1728\n",
      "Epoch 2/15\n",
      "51985/51985 [==============================] - 538s 10ms/step - loss: 2.7769 - acc: 0.2827\n",
      "Epoch 3/15\n",
      "51985/51985 [==============================] - 538s 10ms/step - loss: 2.6093 - acc: 0.3178\n",
      "Epoch 4/15\n",
      "51985/51985 [==============================] - 533s 10ms/step - loss: 2.5489 - acc: 0.3349\n",
      "Epoch 5/15\n",
      "51985/51985 [==============================] - 554s 11ms/step - loss: 2.5177 - acc: 0.3470\n",
      "Epoch 6/15\n",
      "51985/51985 [==============================] - 580s 11ms/step - loss: 2.4951 - acc: 0.3514\n",
      "Epoch 7/15\n",
      "51985/51985 [==============================] - 579s 11ms/step - loss: 2.4825 - acc: 0.3571\n",
      "Epoch 8/15\n",
      "51985/51985 [==============================] - 604s 12ms/step - loss: 2.4758 - acc: 0.3593\n",
      "Epoch 9/15\n",
      "51985/51985 [==============================] - 680s 13ms/step - loss: 2.4893 - acc: 0.3597\n",
      "Epoch 10/15\n",
      "51985/51985 [==============================] - 636s 12ms/step - loss: 2.4834 - acc: 0.3609\n",
      "Epoch 11/15\n",
      "51985/51985 [==============================] - 647s 12ms/step - loss: 2.4805 - acc: 0.3636\n",
      "Epoch 12/15\n",
      "51985/51985 [==============================] - 631s 12ms/step - loss: 2.5226 - acc: 0.3591\n",
      "Epoch 13/15\n",
      "32160/51985 [=================>............] - ETA: 3:55 - loss: 2.5099 - acc: 0.3586 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-940774eeba06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     verbose=1)\n\u001b[0m",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X, \n",
    "    Y,\n",
    "    epochs=N_EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../data/output/models/modellabeled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get largest area and tranform to array\n",
    "\n",
    "def getLargestRegion2(array):\n",
    "    regionmaxprop = None\n",
    "    regions = measure.regionprops(labels)\n",
    "    for regionprop in regions:\n",
    "        # check to see if the region is at least 50% nonzero\n",
    "        if regionmaxprop is None:\n",
    "            regionmaxprop = regionprop\n",
    "        if regionmaxprop.filled_area < regionprop.filled_area:\n",
    "            regionmaxprop = regionprop\n",
    "    return regionmaxprop\n",
    "\n",
    "\n",
    "def regionpropsToArray(rp):\n",
    "    return np.where(labels == rp.label,1.0,0.0)\n",
    "\n",
    "\n",
    "# Final result\n",
    "plt.imshow(regionpropsToArray(getLargestRegion(labelImage(X[4][:,:,0]))), cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
