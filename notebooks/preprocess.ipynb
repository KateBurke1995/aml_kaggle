{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/'\n",
    "TRAIN_IMAGES_LOCATION = '../data/train_images/'\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(DATA_LOCATION + 'train_onelabel.csv')\n",
    "train_labels = train_labels.rename(columns={'image': 'filepath'})\n",
    "#train_labels = train_labels.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(row):\n",
    "    \"\"\"\n",
    "    Load image from filepath to a numpy.ndarray\n",
    "    input:\n",
    "        - filepath: string with relative or absolute path to image\n",
    "    output:\n",
    "        - img:\n",
    "            numpy.ndarray containing the image\n",
    "            shaped (M,N), values [0.0, 1.0]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = TRAIN_IMAGES_LOCATION + row['filepath']\n",
    "        img = mpimg.imread(img)\n",
    "    except:\n",
    "        img = row\n",
    "        img = mpimg.imread(img)\n",
    "    img = np.absolute(np.divide(img.astype(float), 255) - 1.0)\n",
    "    return img\n",
    "\n",
    "def get_padding(i):\n",
    "    \"\"\"\n",
    "    Helper function for getting right padding sizes\n",
    "    input:\n",
    "        - i: positive integer gotten from substracting height and width of an image\n",
    "    output:\n",
    "        - Tuple representing the correct padding\n",
    "    \"\"\"\n",
    "    if i%2 == 0:\n",
    "        return (int(i/2),int(i/2))\n",
    "    else:\n",
    "        return (int(i/2-.5), int(i/2+.5))\n",
    "    \n",
    "def pad_image(img):\n",
    "    \"\"\"\n",
    "    Add padding to image to make it square\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "    output:\n",
    "        - padded array of shape (N,N)\n",
    "    \"\"\"\n",
    "    H, W = img.shape\n",
    "    if H == W:\n",
    "        return img\n",
    "    elif H > W:\n",
    "        return np.pad(img, ((0,0), get_padding(H-W)), 'constant')\n",
    "    else:\n",
    "        return np.pad(img, (get_padding(W-H), (0,0)), 'constant')\n",
    "    \n",
    "def resize_image(img, size):\n",
    "    \"\"\"\n",
    "    Resize image to new square shape\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "        - size: final shape of image in pixels (integer)\n",
    "    \"\"\"\n",
    "    return resize(img, (size,size), mode='reflect')\n",
    "\n",
    "def flattened_image(row):\n",
    "    \"\"\"\n",
    "    Loads and processes image to be used later on\n",
    "    input:\n",
    "        - row: Pandas.DataFrame row\n",
    "    output:\n",
    "        - Python list, flattened np.ndarray\n",
    "    \"\"\"\n",
    "    img = get_image(row)\n",
    "    img = pad_image(img)\n",
    "    img = resize_image(img, IMAGE_SIZE)\n",
    "    return img.flatten().tolist()\n",
    "\n",
    "def get_shape(row):\n",
    "    \"\"\"\n",
    "    Loads and processes image to be used later on\n",
    "    input:\n",
    "        - row: Pandas.DataFrame row\n",
    "    output:\n",
    "        - tuple, with original image dimensions\n",
    "    \"\"\"\n",
    "    img = get_image(row)\n",
    "    return img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get image from file\n",
    "# pad the image to a sqaure\n",
    "# resize to IMAGE_SIZE\n",
    "# flatten and convert np.array to Python list\n",
    "train_labels['image'] = train_labels.apply(flattened_image, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13017</th>\n",
       "      <td>102591.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>133590.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17553</th>\n",
       "      <td>81551.jpg</td>\n",
       "      <td>84</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>50201.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>55875.jpg</td>\n",
       "      <td>45</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15987</th>\n",
       "      <td>13444.jpg</td>\n",
       "      <td>76</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7034</th>\n",
       "      <td>65955.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>153090.jpg</td>\n",
       "      <td>28</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16539</th>\n",
       "      <td>84596.jpg</td>\n",
       "      <td>81</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15774</th>\n",
       "      <td>33396.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filepath  class                                              image\n",
       "13017  102591.jpg     58  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3668   133590.jpg     12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "17553   81551.jpg     84  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2425    50201.jpg      8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "9925    55875.jpg     45  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "15987   13444.jpg     76  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "7034    65955.jpg     31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "6693   153090.jpg     28  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "16539   84596.jpg     81  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "15774   33396.jpg     74  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x192d5e518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5df6dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = train_labels.sample(n=1).iloc[0]['image']\n",
    "plt.imshow(np.asarray(example).reshape(IMAGE_SIZE,IMAGE_SIZE),cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the labels from integer to categorical data\n",
    "train_labels_one_hot = to_categorical(train_labels['class'])\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_labels['class'])\n",
    "nClasses = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_labels['image'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204,), (64, 64)\n",
      "(24204, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i] = np.asarray(train_data[i])\n",
    "\n",
    "print('{}, {}'.format(train_data.shape, train_data[0].shape))\n",
    "train_data = np.array(train_data.tolist())\n",
    "print('{}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24204/24204 [==============================] - 16s - loss: 3.5255 - acc: 0.2141    \n",
      "Epoch 2/20\n",
      "24204/24204 [==============================] - 17s - loss: 2.8258 - acc: 0.3230    \n",
      "Epoch 3/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.5143 - acc: 0.3714    \n",
      "Epoch 4/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.2936 - acc: 0.4078    \n",
      "Epoch 5/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.1443 - acc: 0.4307    \n",
      "Epoch 6/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.0371 - acc: 0.4556    \n",
      "Epoch 7/20\n",
      "24204/24204 [==============================] - 17s - loss: 1.9290 - acc: 0.4776    \n",
      "Epoch 8/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.8414 - acc: 0.4984    \n",
      "Epoch 9/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.7783 - acc: 0.5065    \n",
      "Epoch 10/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.7101 - acc: 0.5213    \n",
      "Epoch 11/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.6428 - acc: 0.5375    \n",
      "Epoch 12/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.5974 - acc: 0.5461    \n",
      "Epoch 13/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.5405 - acc: 0.5616    \n",
      "Epoch 14/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.5120 - acc: 0.5710    \n",
      "Epoch 15/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.4696 - acc: 0.5799    \n",
      "Epoch 16/20\n",
      "24204/24204 [==============================] - 19s - loss: 1.4340 - acc: 0.5887    \n",
      "Epoch 17/20\n",
      "24204/24204 [==============================] - 17s - loss: 1.3930 - acc: 0.5990    \n",
      "Epoch 18/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.3624 - acc: 0.6047    \n",
      "Epoch 19/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.3277 - acc: 0.6134    \n",
      "Epoch 20/20\n",
      "24204/24204 [==============================] - 16s - loss: 1.2950 - acc: 0.6252    \n"
     ]
    }
   ],
   "source": [
    "model_reg = Sequential()\n",
    "model_reg.add(Dense(512, activation='relu', input_shape=(IMAGE_SIZE**2,)))\n",
    "model_reg.add(Dropout(0.5))\n",
    "model_reg.add(Dense(512, activation='relu'))\n",
    "model_reg.add(Dropout(0.5))\n",
    "model_reg.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "model_reg.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_reg = model_reg.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_reg.save('../data/output/models/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "#train_data = train_labels['image'].values\n",
    "#for i in range(len(train_data)):\n",
    "#    train_data[i] = np.asarray(train_data[i]).reshape(IMAGE_SIZE,IMAGE_SIZE)\n",
    "#\n",
    "#print('{}'.format(train_data.shape))\n",
    "train_data = np.reshape(train_data, (-1,64,64,1))\n",
    "print('{}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24204/24204 [==============================] - 640s - loss: 3.0315 - acc: 0.2946   \n",
      "Epoch 2/20\n",
      "24204/24204 [==============================] - 633s - loss: 2.2192 - acc: 0.4310   \n",
      "Epoch 3/20\n",
      "24204/24204 [==============================] - 578s - loss: 1.9056 - acc: 0.4887   \n",
      "Epoch 4/20\n",
      "24204/24204 [==============================] - 532s - loss: 1.6646 - acc: 0.5439   \n",
      "Epoch 5/20\n",
      "24204/24204 [==============================] - 558s - loss: 1.4866 - acc: 0.5918   \n",
      "Epoch 6/20\n",
      "24204/24204 [==============================] - 517s - loss: 1.3234 - acc: 0.6272   \n",
      "Epoch 7/20\n",
      "24204/24204 [==============================] - 522s - loss: 1.1788 - acc: 0.6650   \n",
      "Epoch 8/20\n",
      "24204/24204 [==============================] - 541s - loss: 1.0465 - acc: 0.7003   \n",
      "Epoch 9/20\n",
      "24204/24204 [==============================] - 530s - loss: 0.9348 - acc: 0.7289   \n",
      "Epoch 10/20\n",
      "24204/24204 [==============================] - 559s - loss: 0.8507 - acc: 0.7502   \n",
      "Epoch 11/20\n",
      "24204/24204 [==============================] - 596s - loss: 0.7726 - acc: 0.7674   \n",
      "Epoch 12/20\n",
      "24204/24204 [==============================] - 592s - loss: 0.7114 - acc: 0.7839   \n",
      "Epoch 13/20\n",
      "24204/24204 [==============================] - 547s - loss: 0.6368 - acc: 0.8100   \n",
      "Epoch 14/20\n",
      "24204/24204 [==============================] - 606s - loss: 0.6149 - acc: 0.8148   \n",
      "Epoch 15/20\n",
      "24204/24204 [==============================] - 552s - loss: 0.5698 - acc: 0.8289   \n",
      "Epoch 16/20\n",
      "24204/24204 [==============================] - 559s - loss: 0.5425 - acc: 0.8358   \n",
      "Epoch 17/20\n",
      "24204/24204 [==============================] - 551s - loss: 0.5100 - acc: 0.8454   \n",
      "Epoch 18/20\n",
      "24204/24204 [==============================] - 546s - loss: 0.4836 - acc: 0.8508   \n",
      "Epoch 19/20\n",
      "24204/24204 [==============================] - 547s - loss: 0.4809 - acc: 0.8558   \n",
      "Epoch 20/20\n",
      "24204/24204 [==============================] - 587s - loss: 0.4475 - acc: 0.8635   \n"
     ]
    }
   ],
   "source": [
    "model_a = Sequential()\n",
    "model_a.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE,IMAGE_SIZE,1), padding='same', activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "model_a.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model_a.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_a.add(Flatten())\n",
    "model_a.add(Dense(512, activation='relu'))\n",
    "model_a.add(Dropout(0.5))\n",
    "model_a.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "model_a.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_a = model_a.fit(train_data, train_labels_one_hot, batch_size=128, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.save('../data/output/models/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 121)               62073     \n",
      "=================================================================\n",
      "Total params: 16,849,369\n",
      "Trainable params: 16,849,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
