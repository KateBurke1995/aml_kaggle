{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/'\n",
    "TRAIN_IMAGES_LOCATION = '../data/train_images/'\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(DATA_LOCATION + 'train_onelabel.csv')\n",
    "train_labels = train_labels.rename(columns={'image': 'filepath'})\n",
    "#train_labels = train_labels.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(row):\n",
    "    \"\"\"\n",
    "    Load image from filepath to a numpy.ndarray\n",
    "    input:\n",
    "        - filepath: string with relative or absolute path to image\n",
    "    output:\n",
    "        - img:\n",
    "            numpy.ndarray containing the image\n",
    "            shaped (M,N), values [0.0, 1.0]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = TRAIN_IMAGES_LOCATION + row['filepath']\n",
    "        img = mpimg.imread(img)\n",
    "    except:\n",
    "        img = row\n",
    "        img = mpimg.imread(img)\n",
    "    img = np.absolute(np.divide(img.astype(float), 255) - 1.0)\n",
    "    return img\n",
    "\n",
    "def get_padding(i):\n",
    "    \"\"\"\n",
    "    Helper function for getting right padding sizes\n",
    "    input:\n",
    "        - i: positive integer gotten from substracting height and width of an image\n",
    "    output:\n",
    "        - Tuple representing the correct padding\n",
    "    \"\"\"\n",
    "    if i%2 == 0:\n",
    "        return (int(i/2),int(i/2))\n",
    "    else:\n",
    "        return (int(i/2-.5), int(i/2+.5))\n",
    "    \n",
    "def pad_image(img):\n",
    "    \"\"\"\n",
    "    Add padding to image to make it square\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "    output:\n",
    "        - padded array of shape (N,N)\n",
    "    \"\"\"\n",
    "    H, W = img.shape\n",
    "    if H == W:\n",
    "        return img\n",
    "    elif H > W:\n",
    "        return np.pad(img, ((0,0), get_padding(H-W)), 'constant')\n",
    "    else:\n",
    "        return np.pad(img, (get_padding(W-H), (0,0)), 'constant')\n",
    "    \n",
    "def resize_image(img, size):\n",
    "    \"\"\"\n",
    "    Resize image to new square shape\n",
    "    input:\n",
    "        - img: numpy array (2D) representing image\n",
    "        - size: final shape of image in pixels (integer)\n",
    "    \"\"\"\n",
    "    return resize(img, (size,size), mode='reflect')\n",
    "\n",
    "def flattened_image(row):\n",
    "    \"\"\"\n",
    "    Loads and processes image to be used later on\n",
    "    input:\n",
    "        - row: Pandas.DataFrame row\n",
    "    output:\n",
    "        - Python list, flattened np.ndarray\n",
    "    \"\"\"\n",
    "    img = get_image(row)\n",
    "    img = pad_image(img)\n",
    "    img = resize_image(img, IMAGE_SIZE)\n",
    "    return img.flatten().tolist()\n",
    "\n",
    "def get_shape(row):\n",
    "    \"\"\"\n",
    "    Loads and processes image to be used later on\n",
    "    input:\n",
    "        - row: Pandas.DataFrame row\n",
    "    output:\n",
    "        - tuple, with original image dimensions\n",
    "    \"\"\"\n",
    "    img = get_image(row)\n",
    "    return img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get image from file\n",
    "# pad the image to a sqaure\n",
    "# resize to IMAGE_SIZE\n",
    "# flatten and convert np.array to Python list\n",
    "train_labels['image'] = train_labels.apply(flattened_image, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>80093.jpg</td>\n",
       "      <td>18</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15045</th>\n",
       "      <td>64859.jpg</td>\n",
       "      <td>70</td>\n",
       "      <td>[0.00015198950674008517, 0.01140064912683828, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7051</th>\n",
       "      <td>134198.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13301</th>\n",
       "      <td>103639.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>61114.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10332</th>\n",
       "      <td>57204.jpg</td>\n",
       "      <td>46</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>41532.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>51848.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13672</th>\n",
       "      <td>120553.jpg</td>\n",
       "      <td>61</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22452</th>\n",
       "      <td>23773.jpg</td>\n",
       "      <td>110</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filepath  class                                              image\n",
       "4245    80093.jpg     18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "15045   64859.jpg     70  [0.00015198950674008517, 0.01140064912683828, ...\n",
       "7051   134198.jpg     31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "13301  103639.jpg     58  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1131    61114.jpg      3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "10332   57204.jpg     46  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "13235   41532.jpg     58  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3569    51848.jpg     12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "13672  120553.jpg     61  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "22452   23773.jpg    110  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19f38b4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvXmT7RcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b2425e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = train_labels.sample(n=1).iloc[0]['image']\n",
    "plt.imshow(np.asarray(example).reshape(IMAGE_SIZE,IMAGE_SIZE),cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the labels from integer to categorical data\n",
    "train_labels_one_hot = to_categorical(train_labels['class'])\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_labels['class'])\n",
    "nClasses = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_labels['image'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204,), (4096,)\n",
      "(24204, 4096)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    train_data[i] = np.asarray(train_data[i])\n",
    "\n",
    "# format it in such a way Keras can use it\n",
    "print('{}, {}'.format(train_data.shape, train_data[0].shape))\n",
    "train_data = np.array(train_data.tolist())\n",
    "print('{}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24204/24204 [==============================] - 14s - loss: 3.5230 - acc: 0.2146    \n",
      "Epoch 2/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.8193 - acc: 0.3264    \n",
      "Epoch 3/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.5033 - acc: 0.3723    \n",
      "Epoch 4/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.2945 - acc: 0.4102    \n",
      "Epoch 5/20\n",
      "24204/24204 [==============================] - 14s - loss: 2.1407 - acc: 0.4348    \n",
      "Epoch 6/20\n",
      "24204/24204 [==============================] - 13s - loss: 2.0275 - acc: 0.4580    \n",
      "Epoch 7/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.9212 - acc: 0.4774    \n",
      "Epoch 8/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.8433 - acc: 0.4940    \n",
      "Epoch 9/20\n",
      "24204/24204 [==============================] - 15s - loss: 1.7678 - acc: 0.5123    \n",
      "Epoch 10/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.7068 - acc: 0.5207    \n",
      "Epoch 11/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.6580 - acc: 0.5340    \n",
      "Epoch 12/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.5943 - acc: 0.5502    \n",
      "Epoch 13/20\n",
      "24204/24204 [==============================] - 15s - loss: 1.5546 - acc: 0.5614    \n",
      "Epoch 14/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.5076 - acc: 0.5691    \n",
      "Epoch 15/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.4635 - acc: 0.5811    \n",
      "Epoch 16/20\n",
      "24204/24204 [==============================] - 15s - loss: 1.4318 - acc: 0.5875    \n",
      "Epoch 17/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.3974 - acc: 0.5954    \n",
      "Epoch 18/20\n",
      "24204/24204 [==============================] - 14s - loss: 1.3529 - acc: 0.6111    \n",
      "Epoch 19/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.3297 - acc: 0.6126    \n",
      "Epoch 20/20\n",
      "24204/24204 [==============================] - 13s - loss: 1.3070 - acc: 0.6210    \n"
     ]
    }
   ],
   "source": [
    "def model_first(train_data, train_labels_one_hot):\n",
    "    model_reg = Sequential()\n",
    "    model_reg.add(Dense(512, activation='relu', input_shape=(IMAGE_SIZE**2,)))\n",
    "    model_reg.add(Dropout(0.5))\n",
    "    model_reg.add(Dense(512, activation='relu'))\n",
    "    model_reg.add(Dropout(0.5))\n",
    "    model_reg.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "    model_reg.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history_reg = model_reg.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20, verbose=1)\n",
    "    \n",
    "    model_reg.save('../data/output/models/model1.h5')\n",
    "    model_reg.summary()\n",
    "\n",
    "# model_first(train_data, train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24204, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape it to work with Convolutional layers\n",
    "# after this, input layer will always have: input_shape=(IMAGE_SIZE,IMAGE_SIZE,1)\n",
    "train_data = np.reshape(train_data, (-1,64,64,1))\n",
    "print('{}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24204/24204 [==============================] - 194s - loss: 2.9233 - acc: 0.3138   \n",
      "Epoch 2/20\n",
      "24204/24204 [==============================] - 184s - loss: 2.1563 - acc: 0.4378   \n",
      "Epoch 3/20\n",
      "24204/24204 [==============================] - 182s - loss: 1.8523 - acc: 0.5037   \n",
      "Epoch 4/20\n",
      "24204/24204 [==============================] - 174s - loss: 1.6318 - acc: 0.5538   \n",
      "Epoch 5/20\n",
      "24204/24204 [==============================] - 171s - loss: 1.4770 - acc: 0.5928   \n",
      "Epoch 6/20\n",
      "24204/24204 [==============================] - 172s - loss: 1.3362 - acc: 0.6249   \n",
      "Epoch 7/20\n",
      "24204/24204 [==============================] - 199s - loss: 1.1982 - acc: 0.6581   \n",
      "Epoch 8/20\n",
      "24204/24204 [==============================] - 181s - loss: 1.0874 - acc: 0.6878   \n",
      "Epoch 9/20\n",
      "24204/24204 [==============================] - 187s - loss: 0.9886 - acc: 0.7126   \n",
      "Epoch 10/20\n",
      "24204/24204 [==============================] - 200s - loss: 0.8948 - acc: 0.7363   \n",
      "Epoch 11/20\n",
      "24204/24204 [==============================] - 182s - loss: 0.8149 - acc: 0.7608   \n",
      "Epoch 12/20\n",
      "24204/24204 [==============================] - 188s - loss: 0.7373 - acc: 0.7772   \n",
      "Epoch 13/20\n",
      "24204/24204 [==============================] - 185s - loss: 0.6838 - acc: 0.7973   \n",
      "Epoch 14/20\n",
      "24204/24204 [==============================] - 206s - loss: 0.6290 - acc: 0.8090   \n",
      "Epoch 15/20\n",
      "24204/24204 [==============================] - 196s - loss: 0.5730 - acc: 0.8252   \n",
      "Epoch 16/20\n",
      "24204/24204 [==============================] - 179s - loss: 0.5332 - acc: 0.8366   \n",
      "Epoch 17/20\n",
      "24204/24204 [==============================] - 193s - loss: 0.4940 - acc: 0.8498   \n",
      "Epoch 18/20\n",
      "24204/24204 [==============================] - 188s - loss: 0.4649 - acc: 0.8582   \n",
      "Epoch 19/20\n",
      "24204/24204 [==============================] - 207s - loss: 0.4356 - acc: 0.8668   \n",
      "Epoch 20/20\n",
      "24204/24204 [==============================] - 200s - loss: 0.4060 - acc: 0.8761   \n"
     ]
    }
   ],
   "source": [
    "def model_single_conv(train_data, train_labels_one_hot):\n",
    "    # takes roughly 1 hour to train\n",
    "    model_a = Sequential()\n",
    "    model_a.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE,IMAGE_SIZE,1), padding='same', activation='relu'))\n",
    "    model_a.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_a.add(Flatten())\n",
    "    model_a.add(Dense(512, activation='relu'))\n",
    "    model_a.add(Dropout(0.5))\n",
    "    model_a.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "    model_a.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history_a = model_a.fit(train_data, train_labels_one_hot, batch_size=128, epochs=20, verbose=1)\n",
    "\n",
    "    model_a.save('../data/output/models/model3.h5')\n",
    "\n",
    "    model_a.summary()\n",
    "\n",
    "# model_single_conv(train_data, train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_double_conv():\n",
    "    # takes roughly 3 hours to train\n",
    "    model_a = Sequential()\n",
    "    model_a.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE,IMAGE_SIZE,1), padding='same', activation='relu'))\n",
    "    model_a.add(Dropout(0.5))\n",
    "    model_a.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model_a.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_a.add(Flatten())\n",
    "    model_a.add(Dense(512, activation='relu'))\n",
    "    model_a.add(Dropout(0.5))\n",
    "    model_a.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "    model_a.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history_a = model_a.fit(train_data, train_labels_one_hot, batch_size=128, epochs=20, verbose=1)\n",
    "\n",
    "    model_a.save('../data/output/models/model3.h5')\n",
    "\n",
    "    model_a.summary()\n",
    "    \n",
    "# model_double_conv(train_data, train_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
